data:
  dir: 'data/SIAR'
  split_dir: 'split-1_80_10_10'
  sanity_check: False

train:
  debug: True
  pre_train: False
  batch_size: 16
  lr: 1e-3
  max_epochs: 1
  log_every_n_steps: 1
  device: 'cpu'
  # Select loss function: 'base_loss', 'reconstruction_loss', 'pre_train_loss', 'regularized_loss'
  loss_func: 'overall_loss'
  # Select metric to use for comparison: 'MSELoss', 'SSIMLoss'
  metric: 'MSE'
  weight_decay: 0.01
  mask_decay: 1e-4 # 1e-5
  lambda_gt_loss: 1.0
  lambda_decomp_loss: 1.0
  lambda_occlusion_difference: 1e-3
  strategy: 'ddp_find_unused_parameters_true' #for cpu   or: 'auto'
  accumulate_grad_batches: 4

model:
  checkpoint: False
  upsampler_gt: 'swin' # upsampler for ground truth
  upsampler_sl: 'swin' # upsampler for shadow and light
  upsampler_ob: 'swin' # upsampler for objects (masks + rgb)
  swin:
    pretrained: ''
    use_checkpoint: False
    patch_size: [2, 4, 4]  
  swin_gt:
    decoder:
        pretrained: None
        pretrained2d: True
        patch_size: [2, 4, 4]  
        in_chans: 3
        out_chans: 3
        embed_dim: 768  # changed
        depths: [1, 1, 1, 1] #[2, 6, 2, 2]  # change?
        num_heads: [24, 12, 6, 3]  # reversed order
        window_size: [2, 7, 7]
        mlp_ratio: 4.0
        qkv_bias: True
        #qk_scale: None
        drop_rate: 0.0
        attn_drop_rate: 0.0
        drop_path_rate: 0.2
        #norm_layer: nn.LayerNorm
        patch_norm: False
        frozen_stages: -1
        use_checkpoint: False
  swin_sl:
    decoder:
      pretrained: None
      pretrained2d: True
      patch_size: [2, 4, 4] 
      in_chans: 3
      out_chans: 2
      embed_dim: 768  
      depths: [1, 1, 1, 1] #[2, 6, 2, 2]  
      num_heads: [24, 12, 6, 3] 
      window_size: [2, 7, 7]
      mlp_ratio: 4.0
      qkv_bias: True
      #qk_scale: None
      drop_rate: 0.0
      attn_drop_rate: 0.0
      drop_path_rate: 0.2
      #norm_layer: nn.LayerNorm
      patch_norm: False
      frozen_stages: -1
      use_checkpoint: False
  swin_ob:
    decoder:
      pretrained: None
      pretrained2d: True
      patch_size: [2, 4, 4] 
      in_chans: 3
      out_chans: 4
      embed_dim: 768  
      depths: [1, 1, 1, 1] #[2, 6, 2, 2]  
      num_heads: [24, 12, 6, 3]  
      window_size: [2, 7, 7]
      mlp_ratio: 4.0
      qkv_bias: True
      #qk_scale: None
      drop_rate: 0.0
      attn_drop_rate: 0.0
      drop_path_rate: 0.2
      #norm_layer: nn.LayerNorm
      patch_norm: False
      frozen_stages: -1
      use_checkpoint: False

  
  

